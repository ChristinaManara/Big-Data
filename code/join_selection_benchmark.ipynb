{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************   PHYSICAL PLAN OF JOIN EXPERIMENT WITH OPTIMIZER JOIN SELECTION (DISABLED = False)\n",
      "== Physical Plan ==\n",
      "*(2) Project [song_id#29, song_title#30, rank#31, date#32, region_id#33, chart#34, chart_traffic#35, streams#36, year#37, month#38, day#39, only_date#40, region_name#54]\n",
      "+- *(2) BroadcastHashJoin [region_id#33], [region_id#53], Inner, BuildRight\n",
      "   :- *(2) Project [song_id#29, song_title#30, rank#31, date#32, region_id#33, chart#34, chart_traffic#35, streams#36, year#37, month#38, day#39, only_date#40]\n",
      "   :  +- *(2) Filter isnotnull(region_id#33)\n",
      "   :     +- *(2) FileScan parquet [song_id#29,song_title#30,rank#31,date#32,region_id#33,chart#34,chart_traffic#35,streams#36,year#37,month#38,day#39,only_date#40] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/files/charts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(region_id)], ReadSchema: struct<song_id:int,song_title:string,rank:int,date:timestamp,region_id:int,chart:string,chart_tra...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *(1) Project [region_id#53, region_name#54]\n",
      "         +- *(1) Filter isnotnull(region_id#53)\n",
      "            +- *(1) FileScan parquet [region_id#53,region_name#54] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/files/regions.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(region_id)], ReadSchema: struct<region_id:int,region_name:string>\n",
      "*****************   PHYSICAL PLAN OF JOIN EXPERIMENT WITH OPTIMIZER JOIN SELECTION (DISABLED = True)\n",
      "== Physical Plan ==\n",
      "*(5) Project [song_id#98, song_title#99, rank#100, date#101, region_id#102, chart#103, chart_traffic#104, streams#105, year#106, month#107, day#108, only_date#109, region_name#123]\n",
      "+- *(5) SortMergeJoin [region_id#102], [region_id#122], Inner\n",
      "   :- *(2) Sort [region_id#102 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(region_id#102, 200)\n",
      "   :     +- *(1) Project [song_id#98, song_title#99, rank#100, date#101, region_id#102, chart#103, chart_traffic#104, streams#105, year#106, month#107, day#108, only_date#109]\n",
      "   :        +- *(1) Filter isnotnull(region_id#102)\n",
      "   :           +- *(1) FileScan parquet [song_id#98,song_title#99,rank#100,date#101,region_id#102,chart#103,chart_traffic#104,streams#105,year#106,month#107,day#108,only_date#109] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/files/charts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(region_id)], ReadSchema: struct<song_id:int,song_title:string,rank:int,date:timestamp,region_id:int,chart:string,chart_tra...\n",
      "   +- *(4) Sort [region_id#122 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(region_id#122, 200)\n",
      "         +- *(3) Project [region_id#122, region_name#123]\n",
      "            +- *(3) Filter isnotnull(region_id#122)\n",
      "               +- *(3) FileScan parquet [region_id#122,region_name#123] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://master:9000/files/regions.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(region_id)], ReadSchema: struct<region_id:int,region_name:string>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import sys, time\n",
    "\n",
    "spark = SparkSession.builder.appName('study-joins').getOrCreate()\n",
    "\n",
    "\n",
    "HDFS_PATH = \"hdfs://master:9000/files\"\n",
    "\n",
    "\n",
    "'''\n",
    "\tThis functions executed a query performing a join. \n",
    "\tFill the necessary parts of the function to tune the optimizer \n",
    "\tregarding the join selection. \n",
    "\n",
    "\t@arguments:\n",
    "\t\t- disabled: If the \"disabled\" argument is set to \n",
    "\t\t\t    False, the optimizer should not perform join selection. \n",
    "\n",
    "\t@returns: \n",
    "\t\tThe execution time of performing the benchmark join and writing the result\n",
    "\t\tback to HDFS in parquet format\n",
    "\n",
    "\t@TO-DOs:\n",
    "\t\t1. Fill in spark.conf.set the appropriate spark optimizer property and \n",
    "\t\t   property value to disable the optimizer, if user asks so.\n",
    "\t\t2. Set HDFS_PATH to the hdfs folder containing the charts and regions \n",
    "\t\t   parquet files\n",
    "'''\n",
    "\n",
    "def join_experiment(disabled = False):\n",
    "\tif disabled:\n",
    "        \tspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "\n",
    "\tdf = spark.read.format(\"parquet\")\n",
    "\n",
    "\tdf1 = df.load(HDFS_PATH + \"/charts.parquet\")\n",
    "\tdf2 = df.load(HDFS_PATH + \"/regions.parquet\")\n",
    "\n",
    "\tdf1.registerTempTable(\"charts\")\n",
    "\tdf2.registerTempTable(\"regions\")\n",
    "\n",
    "\tsqlString = 'select c.*, r.region_name as region_name from charts as c, regions as r where c.region_id = r.region_id'\n",
    "\n",
    "\tt1 = time.time()\n",
    "\tspark.sql(sqlString).write.mode('overwrite').parquet(HDFS_PATH + \"/joined_with_optimizer_join_selection_\" + str(not disabled) + \".parquet\")\n",
    "\tt2 = time.time()\n",
    "\n",
    "\tprint(\"*****************   PHYSICAL PLAN OF JOIN EXPERIMENT WITH OPTIMIZER JOIN SELECTION (DISABLED = \" + str(disabled) + \")\")\n",
    "\tspark.sql(sqlString).explain()\n",
    "\n",
    "\treturn t2 - t1\n",
    "\n",
    "\n",
    "enabled_time = join_experiment()\n",
    "disabled_time = join_experiment(disabled = True)\n",
    "times = [('Enabled', enabled_time), ('Disabled', disabled_time)]\n",
    "\n",
    "spark.createDataFrame(data=times, schema = ['Optimizer Join Selection', 'Execution Time']).write.mode('overwrite').option('header', 'true').csv(HDFS_PATH + \"/join_experiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
